{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import deque\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation, concatenate\n",
    "from keras.utils import np_utils, normalize, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 3\n",
    "FUTURE_SEQ_PREDICT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(arr):\n",
    "    return np.random.shuffle(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train\n",
    "    Trains the given model\n",
    "\n",
    "    Uses EarlyStopping\n",
    "inputs:\n",
    "    model\n",
    "    input data\n",
    "    batch_size\n",
    "    epochs\n",
    "returns:\n",
    "    the trained model\n",
    "'''\n",
    "def train(model, x_note_train, x_len_train, y_note_train, y_len_train, batch_size, epochs):\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir=\"./logs/tensorboard\", histogram_freq=0, write_graph=True, write_grads=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "    earlystopping = EarlyStopping(monitor='output_note_acc', min_delta=0, patience=1, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "    filepath = \"../../model-checkpoint.h5\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "    # checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        {'input_notes': x_note_train, 'input_length': x_len_train},\n",
    "        {'output_note': y_note_train, 'output_length': y_len_train},\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        # validation_data=(\n",
    "        #     [x_test[:,:,0,:], x_test[:,:,1,:]],\n",
    "        #     [y_test[:,0,0,:], y_test[:,0,1,:]]),\n",
    "        callbacks=[tensorboard, checkpoint, earlystopping]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "'''\n",
    "getModel\n",
    "    Creates an LSTM model\n",
    "retuns:\n",
    "    keras model\n",
    "'''\n",
    "def getModel(seqLen):\n",
    "    input_notes = Input(shape=(seqLen,87), name=\"input_notes\")\n",
    "    input_length = Input(shape=(seqLen,53), name=\"input_length\")\n",
    "    inputs = concatenate([input_notes, input_length])\n",
    "\n",
    "    model = LSTM(128, return_sequences=True)(inputs)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = LSTM(128, return_sequences=False)(model)\n",
    "    model = Dropout(0.2)(model)\n",
    "\n",
    "    output_note = Dense(87, activation=\"softmax\", name='output_note')(model)\n",
    "    output_length = Dense(53, activation=\"softmax\", name=\"output_length\")(model)\n",
    "\n",
    "    model = Model(inputs=[input_notes, input_length], outputs=[output_note, output_length])\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "getData\n",
    "inputs:\n",
    "    filepath for the dataset\n",
    "    network input sequence length\n",
    "    network prediction sequence length\n",
    "returns:\n",
    "    x_note, x_len - notes and note duration in an array of length seq_len\n",
    "    y_note, y_len - notes and note duration in an array of length prediction_seq_len\n",
    "'''\n",
    "def getData(input_filepath, seq_len, prediction_seq_len):\n",
    "    # noOfInputData = 100000\n",
    "    x_note = []\n",
    "    x_len = []\n",
    "    y_note = []\n",
    "    y_len = []\n",
    "\n",
    "    for file in glob.glob(f\"{input_filepath}*.npy\"):\n",
    "        # if len(x) >= noOfInputData:\n",
    "        #     break\n",
    "\n",
    "        x_data, y_data = createXY(np.load(file), seq_len, prediction_seq_len)\n",
    "\n",
    "        if x_data == [] or y_data == []:\n",
    "            continue\n",
    "        if x_data.ndim == 3:\n",
    "#             print(np.array(x_data).shape)\n",
    "            x_note += list(x_data[:,:,0])\n",
    "            x_len += list(x_data[:,:,1])\n",
    "            y_note += list(y_data[:,:,0])\n",
    "            y_len += list(y_data[:,:,1])\n",
    "    \n",
    "    x_note = np.array(x_note).reshape(299643, 2, 1)\n",
    "    x_len = np.array(x_len).reshape(299643, 2, 1)\n",
    "    \n",
    "    return x_note, x_len, y_note, y_len\n",
    "\n",
    "\n",
    "'''\n",
    "createXY\n",
    "    gets sequences of data\n",
    "inputs:\n",
    "    np array of notes and duration\n",
    "    network input sequence length\n",
    "    network prediction sequence length\n",
    "returns:\n",
    "    x - notes and note duration in an array of length seq_len\n",
    "    y - notes and note duration in an array of length prediction_seq_len\n",
    "'''\n",
    "def createXY(arr, sequenceLength, predictionLength):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    skipRests = True if arr[0][0] == 0 else False\n",
    "    \n",
    "    i = 0\n",
    "    while i+sequenceLength+predictionLength < len(arr):\n",
    "        \n",
    "        if skipRests:\n",
    "            if arr[i][0] == 0:\n",
    "                i += sequenceLength\n",
    "                continue\n",
    "            else:\n",
    "                skipRest = False\n",
    "        \n",
    "        x.append(arr[i : i+sequenceLength])\n",
    "        y.append(arr[i+sequenceLength : i+sequenceLength+predictionLength])\n",
    "        i += sequenceLength\n",
    "\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(x)\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(y)\n",
    "\n",
    "    return (np.array(x), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fireredninja/anaconda3/envs/generateMusicProject/lib/python3.6/site-packages/ipykernel_launcher.py:94: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "/home/fireredninja/anaconda3/envs/generateMusicProject/lib/python3.6/site-packages/ipykernel_launcher.py:94: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299643, 2, 1)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "input_filepath = \"../../data/processed-130000/\"\n",
    "seq_len = 2\n",
    "prediction_seq_len =  1\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "x_note, x_len, y_note, y_len = getData(input_filepath, seq_len, prediction_seq_len)\n",
    "print(np.array(x_note).shape)\n",
    "np.save(\"s2p1_full/x_note\", x_note)\n",
    "np.save(\"s2p1_full/x_len\", x_len)\n",
    "np.save(\"s2p1_full/y_note\", y_note)\n",
    "np.save(\"s2p1_full/y_len\", y_len)\n",
    "\n",
    "# x_note = to_categorical(x_note, 87)\n",
    "# x_len = to_categorical(x_len, 53)\n",
    "# y_note = to_categorical(y_note, 87)\n",
    "# y_len = to_categorical(y_len, 53)\n",
    "\n",
    "\n",
    "# # 80/20 split for training/testing dataset\n",
    "# trainLength = int(round((len(x_note) * 4)/5))\n",
    "# x_note_train = x_note[:trainLength]\n",
    "# x_len_train = x_len[:trainLength]\n",
    "# x_note_test = x_note[trainLength:]\n",
    "# x_len_test = x_len[trainLength:]\n",
    "\n",
    "# y_note_train = y_note[:trainLength]\n",
    "# y_len_train = y_len[:trainLength]\n",
    "# y_note_test = y_note[trainLength:]\n",
    "# y_len_test = y_len[trainLength:]\n",
    "\n",
    "# # NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "# print('train samples', x_note_train.shape)\n",
    "# print('test samples', x_note_test.shape)\n",
    "\n",
    "# model = getModel(seq_len)\n",
    "# model.summary()\n",
    "\n",
    "# model = train(model, x_note_train, x_len_train, y_note_train, y_len_train, batch_size, epochs)\n",
    "\n",
    "# # plot_model(model, to_file='model.png')\n",
    "\n",
    "# # Score model\n",
    "# score = model.evaluate([x_note_test, x_len_test], [y_note_test, y_len_test], verbose=1)\n",
    "# i = 0\n",
    "# while i < len(model.metrics_names):\n",
    "#     print(f\"{model.metrics_names[i]}:\",score[i])\n",
    "#     i+=1\n",
    "# # Save model\n",
    "# model_name = \"../../models/model-test5.hdf5\"\n",
    "# model.save(model_name)\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./numpyRepresentation/\"\n",
    "noOfInputData = 100000\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for file in glob.glob(f\"{path}*.npy\"):\n",
    "    if len(x) >= noOfInputData:\n",
    "        break\n",
    "    \n",
    "    x_data, y_data = createXY(np.load(file), 3, 1)\n",
    "#     print(x_data.ndim)\n",
    "    if x_data.ndim == 3:\n",
    "        x += list(x_data)\n",
    "        y += list(y_data)\n",
    "    \n",
    "x = to_categorical(x)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 split for training/testing dataset\n",
    "trainLength = int(round((len(x) * 4)/5))\n",
    "x_train = x[:trainLength]\n",
    "x_test = x[trainLength:]\n",
    "y_train = y[:trainLength]\n",
    "y_test = y[trainLength:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(x_train[:,:,1,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from time import time\n",
    "\n",
    "\n",
    "input_notes = Input(shape=(3,120), name=\"input_notes\")\n",
    "input_length = Input(shape=(3,120), name=\"input_length\")\n",
    "inputs = concatenate([input_notes, input_length])\n",
    "\n",
    "model = LSTM(128, return_sequences=True)(inputs)\n",
    "model = Dropout(0.2)(model)\n",
    "model = LSTM(128, return_sequences=False)(model)\n",
    "model = Dropout(0.2)(model)\n",
    "\n",
    "output_note = Dense(114, activation=\"softmax\", name='output_note')(model)\n",
    "output_length = Dense(114, activation=\"softmax\", name=\"output_length\")(model)\n",
    "\n",
    "model = Model(inputs=[input_notes, input_length], outputs=[output_note, output_length])\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=x_train.shape[1:], return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256, return_sequences=False))\n",
    "# model.add(Dropout(0.2))\n",
    "# # model.add(Dense(32))\n",
    "# # model.add(Reshape((1, 128)))\n",
    "# model.add(Dense(2, activation='softmax'))\n",
    "# model.compile(\n",
    "#     loss='categorical_crossentropy',\n",
    "#     optimizer='adam',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "# model.summary()\n",
    "\n",
    "# plot_model(model, to_file='model.png')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./logs/{}\".format(time()), histogram_freq=0, batch_size=32, write_graph=True, write_grads=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "# checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    {'input_notes': x_train[:,:,0,:], 'input_length': x_train[:,:,1,:]},\n",
    "    {'output_note': y_train[:,0,0,:], 'output_length': y_train[:,0,1,:]},\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(\n",
    "        [x_test[:,:,0,:], x_test[:,:,1,:]],\n",
    "        [y_test[:,0,0,:], y_test[:,0,1,:]]),\n",
    "    callbacks=[tensorboard, checkpoint, earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model\n",
    "score = model.evaluate([x_test[:,:,0,:], x_test[:,:,1,:]], [y_test[:,0,0,:], y_test[:,0,1,:]], verbose=1)\n",
    "i = 0\n",
    "while i < len(model.metrics_names):\n",
    "    print(f\"{model.metrics_names[i]}:\",score[i])\n",
    "    i+=1\n",
    "# Save model\n",
    "model.save(\"models/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training and validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['output_note_acc'])\n",
    "plt.plot(history.history['val_output_note_acc'])\n",
    "plt.plot(history.history['output_length_acc'])\n",
    "plt.plot(history.history['val_output_length_loss'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Note Train', 'Note Test', 'Length Train', 'Length Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['output_note_loss'])\n",
    "plt.plot(history.history['val_output_note_loss'])\n",
    "plt.plot(history.history['output_length_loss'])\n",
    "plt.plot(history.history['val_output_length_loss'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Note Train', 'Note Test', 'Length Train', 'Length Test', \"Val\", \"Val Loss\"], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load(\"../../models/model-2\")\n",
    "data = np.load(\"./numpyRepresentation/Digimon - Butterfly (Piano Version)0.npy\")\n",
    "data = to_categorical(data)\n",
    "data_notes = np.reshape(data[:,0,:], (63, 3 , 103))\n",
    "data_length = np.reshape(data[:,1,:], (63, 3, 103))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict([data_notes, data_length])\n",
    "print(prediction)\n",
    "# np.save(\"./prediction\", np.reshape(prediction, (63,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(prediction.reshape(63,2))\n",
    "def generateNotes(model, sequence):\n",
    "    return model.predict(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    PREDICTION_LENGTH = 20\n",
    "\n",
    "    prediction = np.reshape(prediction, (63,2))\n",
    "\n",
    "    i = 0\n",
    "    input_sequence = np.array([prediction[:SEQ_LEN]])\n",
    "    output_sequence = input_sequence.copy()\n",
    "    # print(input_sequence)\n",
    "    while i < PREDICTION_LENGTH:\n",
    "        generated_notes = generateNotes(model, input_sequence)\n",
    "        output_sequence = np.append(output_sequence, generated_notes, axis=1)\n",
    "        input_sequence = np.append(input_sequence, generated_notes, axis=1)\n",
    "        input_sequence = np.delete(input_sequence, 0, 1)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "arr1 = np.array([[[1, 6], [2, 7 ],[3, 8]]])\n",
    "arr2 = np.array([[[0.34858927,0.6514107 ]]])\n",
    "\n",
    "# print(\"Shape: \", arr1.shape, arr2.shape)\n",
    "\n",
    "pprint(to_categorical(arr1))\n",
    "result = np.append(arr1,arr2, axis=1)\n",
    "# pprint(result)\n",
    "# np.delete(result, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testArr = [[[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0], [0,0,0,0,0,1,0,0,0,0,0,0,0,]],\n",
    "          [[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0], [0,0,0,0,0,1,0,0,0,0,0,0,0,]]]\n",
    "np.array(testArr, dtype=np.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
